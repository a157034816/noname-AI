# 身份猜测、证据与态度系统（身份局）

本文解释本扩展在 **身份局（`get.mode()==="identity"`）** 中的核心机制：  
如何在不读取“真实身份/暗牌”的前提下，通过公开事件记录“证据（evidence）”、维护“主公阵营线索（zhuSignal）”，并在 UI 面板中给出“猜测身份”与解释；以及它如何通过补丁影响 `get.attitude()`（态度）从而影响 AI 的目标选择。

相关源码：

- 证据/软暴露/主公线索事件：`src/ai_persona/events/identity_events.js`
- 身份猜测：`src/ai_persona/guess_identity.js`
- 态度补丁：`src/ai_persona/attitude_patch.js`
- 心智模型写入：`src/ai_persona/memory.js`

扩展文档对齐点：

- `docs/敌友判断因素1.md` 末尾已包含“当前实现规则（扩展）”，本文在其基础上补齐代码级细节与阈值/公式。

---

## 0) 范围与约束（先看）

1. **仅身份局**：本文所有“证据/猜测/软暴露”逻辑均以 `get.mode()==="identity"` 为前提。
2. **仅本地观察者**：证据记录与推断由“本地 AI 观察者”写入自身心智模型（`observer.storage.slqj_ai.memory.*`）。
3. **不读取暗牌/不直接用真实身份做推断**：
   - “推断谁像忠/反/内”主要使用公开行为 + 引擎评分规则（`get.result(...)`、事件链等）。
   - 少量地方会读取 `player.identity` 做**门槛级布尔判断**（例如“忠臣是否已全死”），目的在于更保守地避免误伤，且不对外展示真实身份。

---

## 1) 两条主线：evidence 与 zhuSignal

扩展同时维护两条“线索轴”：

### 1.1 evidence（证据，主观）
`memory.evidence[pid]` 是“观察者视角”的主观证据：

- 对主忠侧观察者：正值更像友（主忠），负值更像敌（反）
- 对反贼观察者：正值更像友（反），负值更像敌（主忠）

证据来源主要是“单目标行为”的正负收益（tv）以及“我认为目标是友/敌”的关系判断。

### 1.2 zhuSignal（主公阵营线索，偏客观）
`memory.zhuSignal[pid]` 是“更像忠 / 更像反”的线索（+忠，-反）：

- 来源更偏向**公开且强相关**的事件：伤害主公、治疗主公、对主公出杀倾向、对已暴露目标的有益/有害行为等
- 同时累积 `zhuHelp`（正向累计）与 `zhuHarm`（负向累计的绝对值），用于识别“内奸两边摇摆”

身份猜测时，扩展会在 `zhuSignal` 与 `evidence（转换到主公阵营轴）` 中选择更强的一个，避免同一事件双计数。

---

## 2) evidence 如何计算：从“行为收益 tv”到“证据增量”

### 2.1 tv 是什么？
tv 是扩展对“某行为对某目标的收益/伤害”的数值近似：

- 用牌：来自 `get.result(card, skill).target_use/target`
- 用技能：来自 `skill.ai.result.target_use/target`
- 拆/顺：按“被拆/被顺的牌所在区域”专门计算 tv（见 2.4）
- 无懈：解析无懈链后对原 tv 做奇偶层数反转（见 2.3）

扩展只使用引擎的公开评估规则，不会通过窥视暗牌来计算 tv。

### 2.2 “关系方向”：observer 如何判断 target 是友是敌？
`identity_events.js` 定义了一个“关系阈值”体系（用于把行为折算为证据）：

- `REL_ATTITUDE_THRESHOLD = 0.3`
- `REL_ATTITUDE_FULL = 0.6`

规则（简化描述）：

- 观察者计算 `att = get.attitude(observer, target)`
- 若 `|att| <= 0.3`：认为关系不明确（sign=0），不记录证据
- 若 `|att| > 0.3`：关系方向 sign=sign(att)，并计算“确信度” conf：
  - `conf = clamp((|att|-0.3) / (0.6-0.3), 0, 1)`
  - 越接近 0.3 越不确信（conf→0）；到 0.6 及以上则 conf≈1

### 2.3 pushEvidenceByAction：证据增量公式
当扩展得到某次行为的 `tv` 后，会调用内部函数 `pushEvidenceByAction(observer, actor, target, tv, get)`：

- `action = +1` 若 `tv > 0`（帮助目标）
- `action = -1` 若 `tv < 0`（伤害目标）
- `baseMag = clamp(|tv| * 0.6, 0, 1.2)`
- `mag = baseMag * conf`
- `evidenceDelta = sign * action * mag`
  - 若 target 更像友（sign=+1）：帮友加分，打友减分
  - 若 target 更像敌（sign=-1）：帮敌减分，打敌加分

最终写入：`addEvidence(observer, actor, evidenceDelta)` → `observer.memory.evidence[actorPid] += ...`

### 2.4 拆/顺 tv：按区域细化（rewrite 阶段）
为避免“用牌事件”与“选牌结果”重复计数，【过河拆桥】与【顺手牵羊】不在 `useCardToTargeted` 阶段记录，而在结果阶段细化：

- `rewriteDiscardResult`（过河拆桥）
- `rewriteGainResult`（顺手牵羊）

`getCardMoveEvidenceTv(pos, card, target, get)` 规则：

1) 判定区（j）  
拆掉恶意延时牌视为帮助目标：`tv = +1`，支持：

- `lebu`（乐不思蜀）
- `bingliang`（兵粮寸断）
- `shandian`（闪电）
- `fulei`（伏雷）

2) 手牌/装备区（h/e）  
视为伤害目标：`tv < 0`，幅度参考 `get.value`：

- 若是公开/可见（装备区，或已明示手牌）：读取 `get.value(card, target)`
- 若是暗手牌：默认按中性价值 `value=5`
- `tvAbs = clamp(value/5, 0.8, 2.0)`，返回 `tv = -tvAbs`

例外：非满血目标的【白银狮子】（`baiyin`）被拆/被顺视为中立 `tv=0`（既不算善意也不算恶意）。

### 2.5 无懈 tv：支持链式无懈（无懈无懈）
当 `useCardToTargeted` 的牌为 `wuxie` 时：

- 扩展向上解析 `info_map/_info_map` 得到“原始被无懈的牌信息”
- 统计无懈层数 depth（当前无懈计为 1）
- 计算原行为对目标的 `tv0`
- 按奇偶层数反转：
  - depth 为奇数：`tv = -tv0`（取消原效果）
  - depth 为偶数：`tv = tv0`（取消取消，还原原效果）

随后用 `pushEvidenceByAction(observer, wuxie使用者, 原目标, tv, get)` 记录证据。

---

## 3) evidence 的触发点（事件清单）

证据写入主要发生在 `identity_events.js`，由全局技能触发（见 `src/ai_persona/skills/persona_skills.js`）：

### 3.1 单目标用牌（useCardToTargeted）
`onUseCardToTargetedEvidence(trigger, player, game, get, _status)`：

- 条件：
  - identity 模式
  - 单目标（`targets.length===1`）
  - 排除群体/全体牌（`selectTarget:-1`）
  - 排除 guohe/shunshou（交给 rewrite 阶段）
- 处理：
  - 若为 `wuxie`：走无懈链逻辑（2.5）
  - 否则：取 `tv=get.result(...).target_use/target`，调用 `pushEvidenceByAction`

### 3.2 单目标用技能（useSkill）
`onUseSkillEvidence(trigger, player, game, get, _status)`：

- 条件：
  - identity 模式
  - 单目标
  - 技能必须提供 `ai.result.target_use/target`（数值或函数）
  - 跳过 `viewAs` 技能（避免与用牌重复计数）
- 处理：
  - `tv=skill.ai.result.target_use/target`
  - 调用 `pushEvidenceByAction`

### 3.3 拆/顺结果（rewrite 阶段）
`onRewriteDiscardResultEvidence` / `onRewriteGainResultEvidence`：

- 仅对 `guohe` / `shunshou` 的 rewrite 结果
- 仅 1 张牌（扩展按单张细化）
- 调用 2.4 的 tv 规则，再 `pushEvidenceByAction`

### 3.4 兜底：damageEnd/recoverEnd（无法关联用牌/用技能）
`onDamageEndEvidenceGeneral` / `onRecoverEndEvidenceGeneral`：

当结算事件无法稳定关联到：

- `trigger.card.name`（用牌链路）
- 或技能 `ai.result.target(_use)`（用技能链路）

则作为兜底，根据“客观结果”给出小幅 tv：

- 伤害：`tv = -min(2, damage)`
- 回复：`tv = +min(2, heal)`

再 `pushEvidenceByAction` 写入证据。

---

## 4) 软暴露（ai.shown）与“间接主公线索”

扩展会在 `useCardToTargeted` 阶段更新“暴露度”（`player.ai.shown`），并在特定条件下把行为折算成 `zhuSignal`：

### 4.1 暴露度更新规则
`setAiShown(player, minValue)`：把 `player.ai.shown` 提升到至少 `minValue`，不降低，且不超过 `0.95`。

`onUseCardToTargetedExpose` 的核心规则：

1) **对已明置目标做有害行为（tv<0）** → 自己“软暴露拉满”  
`setAiShown(player, 0.95)`

2) **对“已软暴露目标（shown>=0.7）”做有益行为（tv>0）** → 自己也软暴露  
`setAiShown(player, 0.85)`

这些规则用于让“队友救人/帮人”在身份未明置时也产生暴露度，从而减少长期 unknown 的局面。

### 4.2 对主公出杀倾向（无需等待造成伤害）
为避免“主公一直闪/免伤导致线索不足”，扩展补充记录：

- 若某人对主公使用【杀】（useCardToTargeted 阶段），每个本地 AI 观察者都会对该人 `addZhuSignal(..., -0.35)`

### 4.3 间接主公线索：对“已暴露目标”的帮助/伤害
当目标满足 `identityShown===true` 或 `ai.shown>=0.7`（软暴露）时：

扩展会把“你对这个目标的有益/有害行为”折算为“你更像忠/更像反”的线索（写入观察者的 `zhuSignal[playerPid]`）：

- 先判断目标阵营倾向 `align`：
  - 若目标身份明置：主忠=+1，反=-1，其它=0
  - 否则：用 `guessIdentityFor(observer,target)` 的猜测（置信度 conf）
- 行为符号 `actSign`：
  - 帮助目标（tv>0）：+1
  - 伤害目标（tv<0）：-1
- 最终增量：
  - `delta = actSign * align * baseMag * 0.6 * conf * revealWeight`
  - `revealWeight`：明置=1；shown>=0.85=0.8；否则（>=0.7）=0.6

直观理解：

- 帮忠 → 更像忠；打忠 → 更像反  
- 帮反 → 更像反；打反 → 更像忠

---

## 5) 身份猜测：guessIdentityFor / guessIdentityConsensus

身份猜测实现于 `src/ai_persona/guess_identity.js`。

### 5.1 基本身份（公开信息）
若目标满足以下任一条件，则直接确定身份（不做猜测）：

- 目标是主公：`identity="zhu", confidence=1`
- 目标已明置：`identity=target.identity, confidence=1`

### 5.2 单观察者猜测：guessIdentityForDetailed 的主流程
对未明置目标，扩展从观察者的 memory 中取：

- `zhuSignal`（以及 `zhuHelp` / `zhuHarm`）
- `evidence`
- `grudge`
- 目标的软暴露值 `shown=target.ai.shown`

关键常量：

- `SIGNAL_THRESHOLD = 1.2`
- `SOFT_EXPOSE_THRESHOLD = 0.7`
- `FAN_CANDIDATE_CONFIDENCE = 0.65`
- `SOFT_ASSIGN_CONFIDENCE = 0.6`

核心步骤（简化）：

1) **软暴露加权**：  
`weight = 1.25 (shown>=0.85) / 1.1 (shown>=0.7) / 1 (else)`

2) **把 evidence 转换到“主公阵营轴”**：  
`evidenceAxis = evidence`（主忠侧）或 `-evidence`（反贼侧），避免“主观证据方向”与“主公阵营轴”混淆。

3) **选择更强的轴**：  
在 `zhuSignal` 与 `evidenceAxis` 中取绝对值更大的一个作为 `axis`（避免同一事件双计数）。

4) **记仇增强（只增强幅度）**：  
当 `axis!=0` 且 `abs(axis)*weight` 略低于阈值时，用 `grudge` 做轻推，减少“全体弃权→unknown”。

5) **内奸识别**：  
若 `help>=1.8 && harm>=1.8`（同时明显帮过也打过主公），推断为 `nei`。

6) **阈值判断**：
- 若 `absEffective < SIGNAL_THRESHOLD` → `unknown`（并给出较低 confidence）
- 若 `axis > 0` → `zhong`（偏忠）
- 若 `axis < 0` → `fan`（偏反）

### 5.3 共识猜测：guessIdentityConsensus（投票聚合）
扩展会收集“已初始化 persona/memory 的本地 AI 玩家”作为观察者集合（含死亡玩家去重）：

- 每个观察者对目标投一票（`guessIdentityFor`）
- `unknown` 视为弃权
- 票权重为该观察者的 `confidence`（clamp 到 [0,1]）

最终：

- 得票最高身份为共识结果
- `confidence = bestScore / totalScore`
- 若无有效投票则返回 `unknown`

### 5.4 软赋友军（soft assigned remaining allies）
当“反贼候选都已暴露”时，扩展会把剩余 unknown 软赋予为忠臣（仅推断层，不改真实身份）：

- 先估算预期反贼数 expectedFan：
  1) 优先读 `game.__slqjAiPersona.cfg.identityCounts.fan`（公开身份牌堆构成）
  2) 兜底：按真实 `player.identity==="fan"` 数量统计（仅用于数量门槛）
  3) 再兜底：按人数启发式（4人1反、5-6人2反、7人3反、8人4反）
- 再统计 exposedFan / hiddenFan：
  - 以 `guessIdentityConsensus(disableSoftAssign=true)` 的结果判断“反贼候选”（fan 且 conf>=0.65）
  - 若候选同时满足 `identityShown===true` 或 `shown>=0.7` 则计为 exposed，否则 hidden
- 触发条件（非常保守）：
  - `exposedFan >= expectedFan && hiddenFan===0`

触发后：

- 若目标“原始共识推断（禁用软赋予）”为 `unknown`，且目标不是 fan 候选
- 则返回 `identity="zhong"`, `confidence=0.6`, `reason="soft_assigned_remaining_allies"`

该结果会在 UI 面板与态度补丁中被识别，用于减少后期误伤/不救。

---

## 6) 态度补丁：get.attitude(from,to) 的“感知态度”

态度补丁见 `src/ai_persona/attitude_patch.js`，它会包装引擎的 `get.attitude(from,to)`：

### 6.1 触发范围

- 联机（`_status.connectMode`）直接回退引擎原态度
- 身份局中，当目标 **未明置身份** 且 `from !== to`：
  - 若 `to.ai.shown >= 0.85`（软暴露很高）则回退引擎原态度（避免长期中立）
  - 否则可用“感知态度”替代原态度（前提：from 已有 persona+memory）

### 6.2 感知态度的组成（computePerceivedAttitude）
感知态度使用观察者的心智模型：

- impression（第一印象）
- evidence（证据，洞察加权）
- grudge（仇恨，记仇权重加权）
- aggressiveness（激进带来“未知偏敌”）
- 回合推进 certainty（越后期越确信）
- 简单局势缩放（血量/手牌多→更激进）

并将结果 clamp 到 `[-10, 10]`。

### 6.3 伪装型（camouflage）的主公敌意压制
当人格为 camouflage 且自己真实身份为反贼时：

- 前 `traits.camouflageRounds` 个“已行动回合”（turnsTaken）内
- 对主公的负态度会被按进度从强压制逐步恢复（更像“装忠/不跳反”）

### 6.4 软赋友军对态度的影响
若 `explainGuessIdentityFor(from,to)` 的 reason 为 `soft_assigned_remaining_allies`：

- 对主忠侧：把 unknown 软抬为“更友好”（result 至少 3）
- 对反贼侧：把 unknown 软压为“更敌对”（result 至多 -3）

目的：当反贼已全部暴露后，剩余未知更可能是忠/内，减少误伤与不救。

### 6.5 slqj_ai_attitude hook（对外插入点）
若存在订阅者，补丁会 emit `slqj_ai_attitude`，ctx 字段包含：

- `from`/`to`
- `mode`
- `base`（引擎原态度）
- `perceived`（若计算了感知态度）
- `reason`（original / camouflage / fallback_shown / perceived / soft_assigned 等）
- `result`（当前输出态度，可被改写）
- `forceOriginal`（true 则强制返回 base）
- `stop`（终止后续 hook handler）

外部可通过该 hook 改写态度（例如更激进/更保守的推断方式）。

